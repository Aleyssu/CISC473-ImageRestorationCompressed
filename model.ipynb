{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cad1dcc",
   "metadata": {},
   "source": [
    "Download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e281289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just manually download the weights from https://drive.google.com/uc?id=14Fht1QQJ2gMlk4N1ERCRuElg8JfjrWWR and put them in the models folder\n",
    "\n",
    "# import gdown\n",
    "# gdown.download('https://drive.google.com/uc?id=14Fht1QQJ2gMlk4N1ERCRuElg8JfjrWWR', \"./models\", quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8e14f",
   "metadata": {},
   "source": [
    "Init image processing functions and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cad256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from basicsr.utils import img2tensor as _img2tensor, tensor2img, imwrite                                                                                                                                                                                  \n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from basicsr.models import create_model\n",
    "from basicsr.utils.options import parse\n",
    "\n",
    "def imread(img_path):\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  return img\n",
    "def img2tensor(img, bgr2rgb=False, float32=True):\n",
    "    img = img.astype(np.float32) / 255.\n",
    "    return _img2tensor(img, bgr2rgb=bgr2rgb, float32=float32)\n",
    "\n",
    "def display(img1, img2):\n",
    "  fig = plt.figure(figsize=(25, 10))\n",
    "  ax1 = fig.add_subplot(1, 2, 1) \n",
    "  plt.title('Input image', fontsize=16)\n",
    "  ax1.axis('off')\n",
    "  ax2 = fig.add_subplot(1, 2, 2)\n",
    "  plt.title('NAFNet output', fontsize=16)\n",
    "  ax2.axis('off')\n",
    "  ax1.imshow(img1)\n",
    "  ax2.imshow(img2)\n",
    "\n",
    "def single_image_inference(model, img, save_path):\n",
    "      model.feed_data(data={'lq': img.unsqueeze(dim=0)})\n",
    "\n",
    "      if model.opt['val'].get('grids', False):\n",
    "          model.grids()\n",
    "\n",
    "      model.test()\n",
    "\n",
    "      if model.opt['val'].get('grids', False):\n",
    "          model.grids_inverse()\n",
    "\n",
    "      visuals = model.get_current_visuals()\n",
    "      sr_img = tensor2img([visuals['result']])\n",
    "      imwrite(sr_img, save_path)\n",
    "\n",
    "\n",
    "# Load the model\n",
    "opt_path = 'models/NAFNet-width64.yml'\n",
    "opt = parse(opt_path, is_train=False)\n",
    "opt['dist'] = False\n",
    "model = create_model(opt)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb57a03",
   "metadata": {},
   "source": [
    "Demo with a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'demo_images/noisy.png'\n",
    "output_path = 'demo_output/noisy.png'\n",
    "\n",
    "img_input = imread(input_path)\n",
    "inp = img2tensor(img_input)\n",
    "single_image_inference(model, inp, output_path)\n",
    "img_output = imread(output_path)\n",
    "display(img_input, img_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
