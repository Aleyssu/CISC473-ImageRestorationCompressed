{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cad1dcc",
   "metadata": {},
   "source": [
    "Download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e281289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually download the weights from https://drive.google.com/uc?id=14Fht1QQJ2gMlk4N1ERCRuElg8JfjrWWR and put them in the models folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8e14f",
   "metadata": {},
   "source": [
    "Init image processing functions and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cad256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from basicsr.utils import img2tensor as _img2tensor, tensor2img, imwrite                                                                                                                                                                                  \n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from basicsr.models import create_model\n",
    "from basicsr.utils.options import parse\n",
    "\n",
    "def imread(img_path):\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  return img\n",
    "def img2tensor(img, bgr2rgb=False, float32=True):\n",
    "    img = img.astype(np.float32) / 255.\n",
    "    return _img2tensor(img, bgr2rgb=bgr2rgb, float32=float32)\n",
    "\n",
    "def display(img1, img2):\n",
    "  fig = plt.figure(figsize=(25, 10))\n",
    "  ax1 = fig.add_subplot(1, 2, 1) \n",
    "  plt.title('Input image', fontsize=16)\n",
    "  ax1.axis('off')\n",
    "  ax2 = fig.add_subplot(1, 2, 2)\n",
    "  plt.title('NAFNet output', fontsize=16)\n",
    "  ax2.axis('off')\n",
    "  ax1.imshow(img1)\n",
    "  ax2.imshow(img2)\n",
    "\n",
    "def single_image_inference(model, img, save_path):\n",
    "      model.feed_data(data={'lq': img.unsqueeze(dim=0)})\n",
    "\n",
    "      if model.opt['val'].get('grids', False):\n",
    "          model.grids()\n",
    "\n",
    "      model.test()\n",
    "\n",
    "      if model.opt['val'].get('grids', False):\n",
    "          model.grids_inverse()\n",
    "\n",
    "      visuals = model.get_current_visuals()\n",
    "      sr_img = tensor2img([visuals['result']])\n",
    "      imwrite(sr_img, save_path)\n",
    "\n",
    "# Load the model\n",
    "opt_path = 'models/NAFNet-width64.yml'\n",
    "opt = parse(opt_path, is_train=False)\n",
    "opt['dist'] = False\n",
    "model = create_model(opt)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fda88c",
   "metadata": {},
   "source": [
    "Establish model compression parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_amount = 0.5\n",
    "apply_quantization = True\n",
    "apply_half_precision = False\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f11ee5",
   "metadata": {},
   "source": [
    "Prune model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "if not prune_amount == 0:\n",
    "    print(f\"Pruning {prune_amount*100}% of weights...\")\n",
    "    for module in model.net_g.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=prune_amount)\n",
    "    # Make pruning permanent (optional)\n",
    "    for module in model.net_g.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) and hasattr(module, 'weight_orig'):\n",
    "            prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04967496",
   "metadata": {},
   "source": [
    "Fine-tune Pruned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55114ec",
   "metadata": {},
   "source": [
    "Part 1: Dataset Preparation\n",
    "\n",
    "- Download train set from https://drive.google.com/file/d/1UHjWZzLPGweA9ZczmV8lFSRcIxqiOVJw/view?usp=sharing and extract `train` from the zip into `./datasets/SIDD/Data`\n",
    "\n",
    "- Run `python sidd.py` to crop the train image pairs to 512x512 patches and make the data into lmdb format.\n",
    "\n",
    "- Download evaluation data from https://drive.google.com/file/d/1gZx_K2vmiHalRNOb1aj93KuUQ2guOlLp/view?usp=sharing and extract `SIDD` from the zip into `./datasets` to get `./datasets/SIDD/val/input_crops.lmdb` and `./datasets/SIDD/val/gt_crops.lmdb` in your directory structure.\n",
    "\n",
    "- Ran into problems at this point - basicsr requires linux to run its training functions and setting up GPU acceleration in WSL2 was unsuccessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5aa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c154d4",
   "metadata": {},
   "source": [
    "Apply Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.ao.quantization as quant\n",
    "\n",
    "if apply_quantization:\n",
    "    model.net_g.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    \n",
    "    prep = quant.prepare(model.net_g, inplace=True);\n",
    "    \n",
    "    # model.net_g.to(\"cpu\")  # Switch to cpu for quantization\n",
    "    # model.device = torch.device(\"cpu\")\n",
    "\n",
    "    # model.net_g = quant.convert(prep, inplace=True);\n",
    "\n",
    "elif apply_half_precision:\n",
    "    # Attempting half-precision over quantization\n",
    "    model.net_g.cuda().half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb57a03",
   "metadata": {},
   "source": [
    "Demo with a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'demo_images/noisy.png'\n",
    "output_path = 'demo_output/noisy.png'\n",
    "\n",
    "img_input = imread(input_path)\n",
    "inp = img2tensor(img_input)\n",
    "single_image_inference(model, inp, output_path)\n",
    "img_output = imread(output_path)\n",
    "display(img_input, img_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c297c5b",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d18704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import rename\n",
    "\n",
    "model.save(epoch=-1, current_iter=-1)  \n",
    "# Rename model (defaults to \"net_g_latest.pth\") to a format like pruned30_quantized.pth\n",
    "rename(\"models/net_g_latest.pth\", \"models/%s\" % \"pruned%d%s.pth\" % (prune_amount * 100, \"_quantized\" if apply_quantization else \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
